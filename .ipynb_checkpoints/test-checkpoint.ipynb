{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16c77c-10e1-4339-8de8-eb7f9b866651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_contour_group(contour_group: List) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Create a bounding rectangle that encompasses all contours in a group\n",
    "    \"\"\"\n",
    "    if len(contour_group) == 1:\n",
    "        return cv2.boundingRect(contour_group[0])\n",
    "    \n",
    "    # Get bounding rectangles for all contours\n",
    "    rects = [cv2.boundingRect(contour) for contour in contour_group]\n",
    "    \n",
    "    # Find the encompassing rectangle\n",
    "    min_x = min(rect[0] for rect in rects)\n",
    "    min_y = min(rect[1] for rect in rects)\n",
    "    max_x = max(rect[0] + rect[2] for rect in rects)\n",
    "    max_y = max(rect[1] + rect[3] for rect in rects)\n",
    "    \n",
    "    return min_x, min_y, max_x - min_x, max_y - min_y# Vision Board Box Extractor V4 - Enhanced Grouping Version\n",
    "# This code extracts meaningful objects from vision board images keeping text together\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Helper Functions\n",
    "def base64_to_opencv_image(base64_string: str) -> np.ndarray:\n",
    "    \"\"\"Convert base64 string to OpenCV image\"\"\"\n",
    "    try:\n",
    "        if \",\" in base64_string:\n",
    "            base64_string = base64_string.split(\",\")[1]\n",
    "        \n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        pil_image = Image.open(BytesIO(image_data))\n",
    "        \n",
    "        if pil_image.mode == \"RGBA\":\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        \n",
    "        opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "        return opencv_image\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid base64 image data: {str(e)}\")\n",
    "\n",
    "def image_file_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert image file to base64 string for testing\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "def display_base64_image(base64_string: str, title: str = \"Image\"):\n",
    "    \"\"\"Display base64 encoded image in notebook with smaller size\"\"\"\n",
    "    if \",\" in base64_string:\n",
    "        base64_string = base64_string.split(\",\")[1]\n",
    "    \n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    \n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_dominant_background_color(image: np.ndarray, sample_size: int = 1000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate the dominant background color by sampling edge pixels\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Sample pixels from edges (likely to be background)\n",
    "    edge_pixels = []\n",
    "    \n",
    "    # Top and bottom edges\n",
    "    edge_pixels.extend(image[0:10, :].reshape(-1, 3))\n",
    "    edge_pixels.extend(image[h-10:h, :].reshape(-1, 3))\n",
    "    \n",
    "    # Left and right edges\n",
    "    edge_pixels.extend(image[:, 0:10].reshape(-1, 3))\n",
    "    edge_pixels.extend(image[:, w-10:w].reshape(-1, 3))\n",
    "    \n",
    "    edge_pixels = np.array(edge_pixels)\n",
    "    \n",
    "    # Use k-means to find dominant color\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    if len(edge_pixels) > sample_size:\n",
    "        indices = np.random.choice(len(edge_pixels), sample_size, replace=False)\n",
    "        edge_pixels = edge_pixels[indices]\n",
    "    \n",
    "    if len(edge_pixels) > 0:\n",
    "        kmeans = KMeans(n_clusters=1, random_state=42, n_init=10)\n",
    "        kmeans.fit(edge_pixels)\n",
    "        return kmeans.cluster_centers_[0].astype(np.uint8)\n",
    "    else:\n",
    "        return np.array([200, 200, 200], dtype=np.uint8)  # Default gray\n",
    "\n",
    "def create_object_mask(image: np.ndarray, debug: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a mask that identifies foreground objects, focusing on edge-based detection\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Method 1: Enhanced edge detection (this was working well)\n",
    "    # Use bilateral filter to reduce noise while preserving edges\n",
    "    filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    \n",
    "    # Multi-scale edge detection\n",
    "    edges1 = cv2.Canny(filtered, 30, 80)\n",
    "    edges2 = cv2.Canny(filtered, 50, 120)\n",
    "    edges_combined = cv2.bitwise_or(edges1, edges2)\n",
    "    \n",
    "    # Dilate edges to create solid regions\n",
    "    kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    edges_dilated = cv2.dilate(edges_combined, kernel_dilate, iterations=3)\n",
    "    \n",
    "    # Fill enclosed regions using flood fill approach\n",
    "    # Create a copy for flood filling\n",
    "    h, w = edges_dilated.shape\n",
    "    mask_filled = edges_dilated.copy()\n",
    "    \n",
    "    # Close gaps in the edges first\n",
    "    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "    mask_filled = cv2.morphologyEx(mask_filled, cv2.MORPH_CLOSE, kernel_close, iterations=2)\n",
    "    \n",
    "    # Method 2: Adaptive thresholding to catch objects with uniform colors\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(\n",
    "        filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 8\n",
    "    )\n",
    "    \n",
    "    # Invert adaptive threshold to get objects as white\n",
    "    adaptive_thresh_inv = cv2.bitwise_not(adaptive_thresh)\n",
    "    \n",
    "    # Remove small noise from adaptive threshold\n",
    "    kernel_clean = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    adaptive_thresh_clean = cv2.morphologyEx(adaptive_thresh_inv, cv2.MORPH_OPEN, kernel_clean, iterations=1)\n",
    "    \n",
    "    # Method 3: Combine edge-based and adaptive threshold results\n",
    "    combined_mask = cv2.bitwise_or(mask_filled, adaptive_thresh_clean)\n",
    "    \n",
    "    # Final cleanup - remove very small objects and fill small holes\n",
    "    kernel_final = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel_final, iterations=2)\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel_final, iterations=1)\n",
    "    \n",
    "    if debug:\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.imshow(edges_combined, cmap='gray')\n",
    "        plt.title('Raw Edges')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.imshow(edges_dilated, cmap='gray')\n",
    "        plt.title('Dilated Edges')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.imshow(mask_filled, cmap='gray')\n",
    "        plt.title('Edge Mask with Closing')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.imshow(adaptive_thresh_clean, cmap='gray')\n",
    "        plt.title('Adaptive Threshold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.imshow(combined_mask, cmap='gray')\n",
    "        plt.title('Final Combined Mask')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "def group_nearby_contours(contours: List, min_distance: int = 30, debug: bool = False) -> List[List]:\n",
    "    \"\"\"\n",
    "    Group nearby contours that likely belong to the same text/object using DBSCAN clustering\n",
    "    \"\"\"\n",
    "    if len(contours) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Extract centroids of all contours\n",
    "    centroids = []\n",
    "    for contour in contours:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centroids.append([cx, cy])\n",
    "        else:\n",
    "            # Fallback to bounding rect center\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            centroids.append([x + w//2, y + h//2])\n",
    "    \n",
    "    centroids = np.array(centroids)\n",
    "    \n",
    "    # Use DBSCAN to cluster nearby centroids\n",
    "    # eps is the maximum distance between points in the same cluster\n",
    "    clustering = DBSCAN(eps=min_distance, min_samples=1).fit(centroids)\n",
    "    labels = clustering.labels_\n",
    "    \n",
    "    # Group contours by cluster\n",
    "    groups = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in groups:\n",
    "            groups[label] = []\n",
    "        groups[label].append(contours[i])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Grouped {len(contours)} contours into {len(groups)} clusters\")\n",
    "        for label, group in groups.items():\n",
    "            print(f\"Cluster {label}: {len(group)} contours\")\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def filter_corner_fragments(contour_groups: List[List], debug: bool = False) -> List[List]:\n",
    "    \"\"\"\n",
    "    Remove small corner fragments that are likely parts of larger objects\n",
    "    \"\"\"\n",
    "    if len(contour_groups) <= 1:\n",
    "        return contour_groups\n",
    "    \n",
    "    # Get bounding rectangles for all groups\n",
    "    group_rects = []\n",
    "    for i, group in enumerate(contour_groups):\n",
    "        x, y, w, h = merge_contour_group(group)\n",
    "        area = w * h\n",
    "        group_rects.append((x, y, w, h, area, group, i))\n",
    "    \n",
    "    # Sort by area (largest first)\n",
    "    group_rects.sort(key=lambda item: item[4], reverse=True)\n",
    "    \n",
    "    filtered_groups = []\n",
    "    removed_fragments = []\n",
    "    \n",
    "    for i, (x, y, w, h, area, group, orig_idx) in enumerate(group_rects):\n",
    "        is_fragment = False\n",
    "        \n",
    "        # Increased threshold to catch more potential fragments\n",
    "        if area < 8000:  # Increased from 5000 to 8000\n",
    "            \n",
    "            # Compare with larger objects that were processed earlier\n",
    "            for j in range(i):\n",
    "                larger_x, larger_y, larger_w, larger_h, larger_area, _, larger_orig_idx = group_rects[j]\n",
    "                \n",
    "                # Skip if the larger object is not significantly larger\n",
    "                if larger_area < area * 2:\n",
    "                    continue\n",
    "                \n",
    "                # Check spatial relationship\n",
    "                small_center_x = x + w // 2\n",
    "                small_center_y = y + h // 2\n",
    "                \n",
    "                # Generous margin for detecting nearby fragments\n",
    "                margin = 40\n",
    "                expanded_left = larger_x - margin\n",
    "                expanded_right = larger_x + larger_w + margin\n",
    "                expanded_top = larger_y - margin\n",
    "                expanded_bottom = larger_y + larger_h + margin\n",
    "                \n",
    "                # Check if small object is within the expanded area of larger object\n",
    "                if (expanded_left <= small_center_x <= expanded_right and \n",
    "                    expanded_top <= small_center_y <= expanded_bottom):\n",
    "                    \n",
    "                    # Check distance to corners of the larger rectangle\n",
    "                    corners = [\n",
    "                        (larger_x, larger_y),  # top-left\n",
    "                        (larger_x + larger_w, larger_y),  # top-right\n",
    "                        (larger_x, larger_y + larger_h),  # bottom-left\n",
    "                        (larger_x + larger_w, larger_y + larger_h)  # bottom-right\n",
    "                    ]\n",
    "                    \n",
    "                    min_corner_distance = min(\n",
    "                        ((small_center_x - corner_x) ** 2 + (small_center_y - corner_y) ** 2) ** 0.5\n",
    "                        for corner_x, corner_y in corners\n",
    "                    )\n",
    "                    \n",
    "                    # Check distance to edges\n",
    "                    edge_distances = [\n",
    "                        abs(small_center_x - larger_x),  # distance to left edge\n",
    "                        abs(small_center_x - (larger_x + larger_w)),  # distance to right edge\n",
    "                        abs(small_center_y - larger_y),  # distance to top edge\n",
    "                        abs(small_center_y - (larger_y + larger_h))  # distance to bottom edge\n",
    "                    ]\n",
    "                    \n",
    "                    min_edge_distance = min(edge_distances)\n",
    "                    \n",
    "                    # Fragment detection thresholds\n",
    "                    if min_corner_distance < 50 or min_edge_distance < 25:\n",
    "                        is_fragment = True\n",
    "                        break\n",
    "                    \n",
    "                    # Additional check: if the small object overlaps significantly with larger object\n",
    "                    overlap_x = max(0, min(x + w, larger_x + larger_w) - max(x, larger_x))\n",
    "                    overlap_y = max(0, min(y + h, larger_y + larger_h) - max(y, larger_y))\n",
    "                    overlap_area = overlap_x * overlap_y\n",
    "                    overlap_ratio = overlap_area / area\n",
    "                    \n",
    "                    if overlap_ratio > 0.3:  # If 30% of small object overlaps with larger\n",
    "                        is_fragment = True\n",
    "                        break\n",
    "        \n",
    "        if not is_fragment:\n",
    "            filtered_groups.append(group)\n",
    "        else:\n",
    "            removed_fragments.append((x, y, w, h, orig_idx))\n",
    "    \n",
    "    return filtered_groups\n",
    "    \"\"\"\n",
    "    Create a bounding rectangle that encompasses all contours in a group\n",
    "    \"\"\"\n",
    "    if len(contour_group) == 1:\n",
    "        return cv2.boundingRect(contour_group[0])\n",
    "    \n",
    "    # Get bounding rectangles for all contours\n",
    "    rects = [cv2.boundingRect(contour) for contour in contour_group]\n",
    "    \n",
    "    # Find the encompassing rectangle\n",
    "    min_x = min(rect[0] for rect in rects)\n",
    "    min_y = min(rect[1] for rect in rects)\n",
    "    max_x = max(rect[0] + rect[2] for rect in rects)\n",
    "    max_y = max(rect[1] + rect[3] for rect in rects)\n",
    "    \n",
    "    return min_x, min_y, max_x - min_x, max_y - min_y\n",
    "\n",
    "def is_valid_object(x: int, y: int, w: int, h: int, image_shape: Tuple, debug: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a detected region represents a valid object to extract\n",
    "    \"\"\"\n",
    "    area = w * h\n",
    "    image_area = image_shape[0] * image_shape[1]\n",
    "    \n",
    "    # Size filters - more inclusive than before\n",
    "    min_area = 300  # Reduced minimum area\n",
    "    max_area = image_area * 0.7  # Maximum 70% of image\n",
    "    \n",
    "    if area < min_area:\n",
    "        if debug:\n",
    "            print(f\"Rejected: too small ({area} < {min_area})\")\n",
    "        return False\n",
    "    \n",
    "    if area > max_area:\n",
    "        if debug:\n",
    "            print(f\"Rejected: too large ({area} > {max_area})\")\n",
    "        return False\n",
    "    \n",
    "    # Minimum dimensions - reduced for better text capture\n",
    "    min_width, min_height = 25, 25\n",
    "    if w < min_width or h < min_height:\n",
    "        if debug:\n",
    "            print(f\"Rejected: dimensions too small ({w}x{h})\")\n",
    "        return False\n",
    "    \n",
    "    # Aspect ratio - allow wider range for different text orientations\n",
    "    aspect_ratio = max(w, h) / min(w, h)\n",
    "    if aspect_ratio > 15:  # Only reject extremely thin lines\n",
    "        if debug:\n",
    "            print(f\"Rejected: extreme aspect ratio ({aspect_ratio})\")\n",
    "        return False\n",
    "    \n",
    "    # Additional check: reject objects that are too close to image borders\n",
    "    border_margin = 5\n",
    "    if (x < border_margin or y < border_margin or \n",
    "        x + w > image_shape[1] - border_margin or \n",
    "        y + h > image_shape[0] - border_margin):\n",
    "        if debug:\n",
    "            print(f\"Rejected: too close to border\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def extract_objects_from_vision_board_base64(image_base64: str, debug: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract meaningful objects (text groups, pictures) from a vision board image\n",
    "    \"\"\"\n",
    "    image = base64_to_opencv_image(image_base64)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create mask to identify foreground objects  \n",
    "    mask = create_object_mask(image, debug=debug)\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Found {len(contours)} initial contours\")\n",
    "    \n",
    "    # Filter out very small contours early\n",
    "    min_contour_area = 200  # Reduced threshold\n",
    "    filtered_contours = [c for c in contours if cv2.contourArea(c) > min_contour_area]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"After size filtering: {len(filtered_contours)} contours\")\n",
    "    \n",
    "    # Group nearby contours (this handles the character splitting issue)\n",
    "    contour_groups = group_nearby_contours(filtered_contours, min_distance=30, debug=debug)\n",
    "    \n",
    "    # Filter out corner fragments that are likely parts of larger objects\n",
    "    contour_groups = filter_corner_fragments(contour_groups, debug=debug)\n",
    "    \n",
    "    # Create bounding boxes for each group\n",
    "    object_regions = []\n",
    "    for group in contour_groups:\n",
    "        x, y, w, h = merge_contour_group(group)\n",
    "        \n",
    "        if is_valid_object(x, y, w, h, image_rgb.shape[:2], debug=debug):\n",
    "            object_regions.append((x, y, w, h))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Final valid objects: {len(object_regions)}\")\n",
    "        \n",
    "        # Visualize the detection process\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        \n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.title('Object Detection Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 4, 3)\n",
    "        # Show contours on original image\n",
    "        contour_img = image_rgb.copy()\n",
    "        cv2.drawContours(contour_img, filtered_contours, -1, (255, 0, 0), 2)\n",
    "        plt.imshow(contour_img)\n",
    "        plt.title('Detected Contours')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 4, 4)\n",
    "        result_img = image_rgb.copy()\n",
    "        for i, (x, y, w, h) in enumerate(object_regions):\n",
    "            cv2.rectangle(result_img, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "            cv2.putText(result_img, str(i+1), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        plt.imshow(result_img)\n",
    "        plt.title('Final Extracted Objects')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # Sort by vertical position (top to bottom, then left to right)\n",
    "    object_regions = sorted(object_regions, key=lambda region: (region[1], region[0]))\n",
    "    \n",
    "    # Extract each object region\n",
    "    object_base64_list = []\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(object_regions):\n",
    "        # Add padding around the detected object\n",
    "        padding = 5\n",
    "        x_padded = max(0, x - padding)\n",
    "        y_padded = max(0, y - padding)\n",
    "        w_padded = min(image_rgb.shape[1] - x_padded, w + 2 * padding)\n",
    "        h_padded = min(image_rgb.shape[0] - y_padded, h + 2 * padding)\n",
    "        \n",
    "        # Extract the object\n",
    "        obj = image_rgb[y_padded:y_padded+h_padded, x_padded:x_padded+w_padded]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Extracting object {i+1}: position ({x_padded}, {y_padded}), size ({w_padded}, {h_padded})\")\n",
    "        \n",
    "        # Convert to base64\n",
    "        pil_image = Image.fromarray(obj)\n",
    "        buffer = BytesIO()\n",
    "        pil_image.save(buffer, format='JPEG')\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        object_base64_list.append(f\"data:image/jpeg;base64,{img_base64}\")\n",
    "    \n",
    "    return object_base64_list\n",
    "\n",
    "def process_vision_board(image_base64: str, filename: str = \"image.jpg\", debug: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process vision board image and extract meaningful objects (keeping text together)\n",
    "    \"\"\"\n",
    "    if not image_base64:\n",
    "        raise ValueError(\"No image data provided\")\n",
    "    \n",
    "    session_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        object_base64_list = extract_objects_from_vision_board_base64(image_base64, debug=debug)\n",
    "        \n",
    "        results = []\n",
    "        for i, obj_base64 in enumerate(object_base64_list):\n",
    "            results.append({\n",
    "                \"id\": i + 1,\n",
    "                \"image\": obj_base64\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"session_id\": session_id,\n",
    "            \"filename\": filename,\n",
    "            \"num_objects_found\": len(object_base64_list),\n",
    "            \"objects\": results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Processing error: {str(e)}\")\n",
    "\n",
    "def save_objects_to_files(result_data, output_dir=\"extracted_objects\"):\n",
    "    \"\"\"Save extracted objects as individual image files\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    for obj in result_data['objects']:\n",
    "        # Remove data URL prefix\n",
    "        base64_string = obj['image'].split(\",\")[1] if \",\" in obj['image'] else obj['image']\n",
    "        \n",
    "        # Decode and save\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        filename = f\"{output_dir}/object_{obj['id']}.jpg\"\n",
    "        \n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Testing Section\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Vision Board Object Extractor V4 - Enhanced Grouping\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Update this path to your vision board image\n",
    "    image_path = \"/Users/angelo/downloads/Coding3/refined-extract-boxes/images/IMG_9859.jpeg\"\n",
    "    \n",
    "    try:\n",
    "        # Convert image to base64\n",
    "        image_base64 = image_file_to_base64(image_path)\n",
    "        \n",
    "        # Display original image\n",
    "        print(\"Original Vision Board:\")\n",
    "        display_base64_image(image_base64, \"Original Vision Board\")\n",
    "        \n",
    "        # Process the image with debug enabled\n",
    "        result = process_vision_board(image_base64, \"vision_board.jpg\", debug=True)\n",
    "        \n",
    "        print(f\"\\nProcessing Results:\")\n",
    "        print(f\"Session ID: {result['session_id']}\")\n",
    "        print(f\"Number of objects found: {result['num_objects_found']}\")\n",
    "        \n",
    "        # Display all extracted objects\n",
    "        if result['num_objects_found'] > 0:\n",
    "            print(f\"\\nDisplaying {result['num_objects_found']} extracted objects:\")\n",
    "            \n",
    "            for obj in result['objects']:\n",
    "                display_base64_image(obj['image'], f\"Extracted Object {obj['id']}\")\n",
    "        else:\n",
    "            print(\"No objects detected. Try adjusting the parameters.\")\n",
    "        \n",
    "        # Optionally save objects to files\n",
    "        # saved_files = save_objects_to_files(result)\n",
    "        # print(f\"\\nSaved {len(saved_files)} objects to files\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find image file at {image_path}\")\n",
    "        print(\"Please update the image_path variable with the correct path to your image.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def test_functionality():\n",
    "    \"\"\"Test that all functions are properly defined\"\"\"\n",
    "    functions = [\n",
    "        'base64_to_opencv_image',\n",
    "        'image_file_to_base64', \n",
    "        'display_base64_image',\n",
    "        'get_dominant_background_color',\n",
    "        'create_object_mask',\n",
    "        'group_nearby_contours',\n",
    "        'filter_corner_fragments',\n",
    "        'merge_contour_group',\n",
    "        'is_valid_object',\n",
    "        'extract_objects_from_vision_board_base64',\n",
    "        'process_vision_board',\n",
    "        'save_objects_to_files'\n",
    "    ]\n",
    "    \n",
    "    print(\"Function availability check:\")\n",
    "    for func_name in functions:\n",
    "        if func_name in globals():\n",
    "            print(f\"✓ {func_name} - Available\")\n",
    "        else:\n",
    "            print(f\"✗ {func_name} - Missing\")\n",
    "    \n",
    "    print(\"\\nAll functions loaded successfully!\")\n",
    "    print(\"Ready to process vision board images with enhanced object grouping!\")\n",
    "\n",
    "# Run the test\n",
    "test_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb415924-a4b8-4a68-9012-f1be858543eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
